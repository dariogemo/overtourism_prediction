Args in experiment:
Namespace(activation='gelu', batch_size=64, c_out=1, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='data_palazzo_della_ragione_o_del_comune.csv', dec_in=5, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=5, factor=1, features='MS', freq='t', gpu=0, individual=False, is_training=1, itr=1, label_len=48, learning_rate=0.0001, loss='mse', lradj='type1', model='DLinear', model_id='data_palazzo_della_ragione_o_del_comune', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=7, pred_len=96, root_path='/home/dario/UniversitÃ /Data_science/Secondo_anno-secondo_semestre/Stage/Code/DLinear/../../main_dataset/count_data', seq_len=96, target='count', test_flop=False, train_epochs=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : data_palazzo_della_ragione_o_del_comune_DLinear_custom_ftMS_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 160566
val 22872
test 45835
	iters: 100, epoch: 1 | loss: 1.3510784
	speed: 0.0105s/iter; left time: 25.2968s
	iters: 200, epoch: 1 | loss: 0.8822197
	speed: 0.0048s/iter; left time: 11.1101s
	iters: 300, epoch: 1 | loss: 0.7862123
	speed: 0.0049s/iter; left time: 10.7302s
	iters: 400, epoch: 1 | loss: 0.6778814
	speed: 0.0051s/iter; left time: 10.7138s
	iters: 500, epoch: 1 | loss: 0.6103253
	speed: 0.0052s/iter; left time: 10.4488s
	iters: 600, epoch: 1 | loss: 0.7173228
	speed: 0.0058s/iter; left time: 11.0371s
	iters: 700, epoch: 1 | loss: 0.6222222
	speed: 0.0054s/iter; left time: 9.7549s
	iters: 800, epoch: 1 | loss: 0.6696298
	speed: 0.0046s/iter; left time: 7.8900s
	iters: 900, epoch: 1 | loss: 0.7934531
	speed: 0.0052s/iter; left time: 8.3867s
	iters: 1000, epoch: 1 | loss: 0.6326627
	speed: 0.0053s/iter; left time: 7.9895s
	iters: 1100, epoch: 1 | loss: 0.6464148
	speed: 0.0053s/iter; left time: 7.4970s
	iters: 1200, epoch: 1 | loss: 0.6928458
	speed: 0.0048s/iter; left time: 6.3171s
	iters: 1300, epoch: 1 | loss: 0.6103714
	speed: 0.0046s/iter; left time: 5.6088s
	iters: 1400, epoch: 1 | loss: 0.7183368
	speed: 0.0052s/iter; left time: 5.7632s
	iters: 1500, epoch: 1 | loss: 0.7725453
	speed: 0.0049s/iter; left time: 4.9136s
	iters: 1600, epoch: 1 | loss: 0.8908486
	speed: 0.0047s/iter; left time: 4.2655s
	iters: 1700, epoch: 1 | loss: 0.7715591
	speed: 0.0046s/iter; left time: 3.6905s
	iters: 1800, epoch: 1 | loss: 0.6831754
	speed: 0.0046s/iter; left time: 3.2834s
	iters: 1900, epoch: 1 | loss: 0.7921736
	speed: 0.0049s/iter; left time: 2.9871s
	iters: 2000, epoch: 1 | loss: 0.7906989
	speed: 0.0048s/iter; left time: 2.4228s
	iters: 2100, epoch: 1 | loss: 0.6019512
	speed: 0.0048s/iter; left time: 1.9457s
	iters: 2200, epoch: 1 | loss: 0.7973566
	speed: 0.0047s/iter; left time: 1.4450s
	iters: 2300, epoch: 1 | loss: 0.7547252
	speed: 0.0047s/iter; left time: 0.9856s
	iters: 2400, epoch: 1 | loss: 0.6404482
	speed: 0.0048s/iter; left time: 0.5268s
	iters: 2500, epoch: 1 | loss: 0.5474730
	speed: 0.0050s/iter; left time: 0.0450s
Epoch: 1 cost time: 13.061551809310913
Epoch: 1, Steps: 2508 | Train Loss: 0.7729273 Vali Loss: 0.6156881 Test Loss: 0.3597296
Validation loss decreased (inf --> 0.615688).  Saving model ...
Updating learning rate to 0.0001
>>>>>>>testing : data_palazzo_della_ragione_o_del_comune_DLinear_custom_ftMS_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 45835
mse:0.35973188281059265, mae:0.2942315936088562, rse:0.889883279800415, corr:[0.2841131  0.2888582  0.29465127 0.29453883 0.2947222  0.2921212
 0.29165137 0.28755924 0.28505993 0.28250214 0.27869663 0.27749473
 0.27836302 0.27788273 0.27738863 0.28038746 0.28173658 0.2817839
 0.28241992 0.28013533 0.27999952 0.27774468 0.27717555 0.27531236
 0.27619913 0.27671534 0.27601355 0.27587983 0.27589858 0.27528086
 0.27515575 0.27665398 0.27577543 0.27573237 0.27557474 0.27564457
 0.27559158 0.2755763  0.2748706  0.27503902 0.27603784 0.27641657
 0.2767946  0.27577487 0.27570936 0.27637127 0.275024   0.27544874
 0.2751341  0.27284396 0.27503243 0.27546313 0.27649495 0.27522606
 0.27491733 0.27406624 0.27554345 0.27396336 0.27545777 0.27567214
 0.27484083 0.274991   0.27593127 0.27656403 0.27581424 0.2770187
 0.27486235 0.2751803  0.27446154 0.27635565 0.27526295 0.2769897
 0.27666602 0.27448434 0.27176562 0.2716691  0.27067158 0.2718095
 0.27264765 0.27273875 0.27171025 0.27237168 0.27233565 0.27554494
 0.2756763  0.27657068 0.27772248 0.2800248  0.28091112 0.28295183
 0.28431857 0.2873106  0.29057837 0.29202    0.29672444 0.30038214]
