Args in experiment:
Namespace(activation='gelu', batch_size=64, c_out=1, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='data_castelvecchio.csv', dec_in=5, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=5, factor=1, features='MS', freq='t', gpu=0, individual=False, is_training=1, itr=1, label_len=48, learning_rate=0.0001, loss='mse', lradj='type1', model='DLinear', model_id='data_castelvecchio', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=7, pred_len=96, root_path='/home/dario/UniversitÃ /Data_science/Secondo_anno-secondo_semestre/Stage/Code/DLinear/../../main_dataset/count_data', seq_len=96, target='count', test_flop=False, train_epochs=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : data_castelvecchio_DLinear_custom_ftMS_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 20312
val 2834
test 5763
	iters: 100, epoch: 1 | loss: 0.8366652
	speed: 0.0089s/iter; left time: 1.9329s
	iters: 200, epoch: 1 | loss: 0.6059585
	speed: 0.0050s/iter; left time: 0.5855s
	iters: 300, epoch: 1 | loss: 0.7800602
	speed: 0.0051s/iter; left time: 0.0913s
Epoch: 1 cost time: 2.092519760131836
Epoch: 1, Steps: 317 | Train Loss: 0.6985690 Vali Loss: 0.0126261 Test Loss: 0.0232281
Validation loss decreased (inf --> 0.012626).  Saving model ...
Updating learning rate to 0.0001
>>>>>>>testing : data_castelvecchio_DLinear_custom_ftMS_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5763
mse:0.0232281181961298, mae:0.06940027326345444, rse:1.1051756143569946, corr:[0.02979003 0.02958902 0.02830231 0.03173695 0.03018348 0.03019704
 0.03127931 0.02985317 0.0310366  0.03057681 0.03157153 0.0324668
 0.03249898 0.03336757 0.03347807 0.03340981 0.03330217 0.03308573
 0.03318252 0.03291215 0.03235981 0.03259496 0.03240651 0.03257858
 0.03281535 0.03265462 0.03269175 0.03254468 0.03255523 0.03226149
 0.03235824 0.03228387 0.0326287  0.03244735 0.03262173 0.03293842
 0.03279641 0.03267548 0.03245461 0.03246066 0.03260377 0.0324223
 0.03282415 0.03256548 0.03244691 0.03252488 0.03252256 0.03259802
 0.03256792 0.03269497 0.03246076 0.03275551 0.03274256 0.03263761
 0.0327978  0.0324913  0.032656   0.03241216 0.0325507  0.03265402
 0.03273151 0.03257811 0.03276816 0.03272449 0.03248387 0.03258183
 0.03229337 0.03256014 0.03230989 0.03199256 0.03225832 0.03185676
 0.0322959  0.03240728 0.03232293 0.03204087 0.03222574 0.03233725
 0.03167958 0.02795016 0.02574294 0.02547283 0.02633828 0.0272042
 0.02713647 0.02578549 0.02806635 0.02683275 0.02792101 0.02671578
 0.02826122 0.02674062 0.02888306 0.02701629 0.02785229 0.02712275]
