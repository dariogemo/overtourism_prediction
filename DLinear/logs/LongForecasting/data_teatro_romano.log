Args in experiment:
Namespace(activation='gelu', batch_size=64, c_out=1, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='data_teatro_romano.csv', dec_in=5, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=5, factor=1, features='MS', freq='t', gpu=0, individual=False, is_training=1, itr=1, label_len=48, learning_rate=0.0001, loss='mse', lradj='type1', model='DLinear', model_id='data_teatro_romano', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=7, pred_len=96, root_path='/home/dario/UniversitÃ /Data_science/Secondo_anno-secondo_semestre/Stage/Code/DLinear/../../main_dataset/count_data', seq_len=96, target='count', test_flop=False, train_epochs=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : data_teatro_romano_DLinear_custom_ftMS_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 202148
val 28812
test 57716
	iters: 100, epoch: 1 | loss: 0.7058379
	speed: 0.0124s/iter; left time: 37.8448s
	iters: 200, epoch: 1 | loss: 0.6491582
	speed: 0.0055s/iter; left time: 16.2014s
	iters: 300, epoch: 1 | loss: 0.7252840
	speed: 0.0059s/iter; left time: 16.7589s
	iters: 400, epoch: 1 | loss: 0.7371227
	speed: 0.0051s/iter; left time: 14.1397s
	iters: 500, epoch: 1 | loss: 0.5706915
	speed: 0.0052s/iter; left time: 13.8260s
	iters: 600, epoch: 1 | loss: 0.7565772
	speed: 0.0053s/iter; left time: 13.6529s
	iters: 700, epoch: 1 | loss: 0.8015994
	speed: 0.0052s/iter; left time: 12.8946s
	iters: 800, epoch: 1 | loss: 0.7804946
	speed: 0.0051s/iter; left time: 12.0177s
	iters: 900, epoch: 1 | loss: 0.6087362
	speed: 0.0057s/iter; left time: 12.8824s
	iters: 1000, epoch: 1 | loss: 0.7840941
	speed: 0.0092s/iter; left time: 19.7824s
	iters: 1100, epoch: 1 | loss: 0.8807645
	speed: 0.0060s/iter; left time: 12.4513s
	iters: 1200, epoch: 1 | loss: 0.6403767
	speed: 0.0059s/iter; left time: 11.4988s
	iters: 1300, epoch: 1 | loss: 0.5323764
	speed: 0.0068s/iter; left time: 12.7162s
	iters: 1400, epoch: 1 | loss: 0.5639248
	speed: 0.0056s/iter; left time: 9.9095s
	iters: 1500, epoch: 1 | loss: 0.6228808
	speed: 0.0061s/iter; left time: 10.0600s
	iters: 1600, epoch: 1 | loss: 0.7573063
	speed: 0.0052s/iter; left time: 8.1593s
	iters: 1700, epoch: 1 | loss: 0.8903763
	speed: 0.0055s/iter; left time: 8.0055s
	iters: 1800, epoch: 1 | loss: 0.7584543
	speed: 0.0059s/iter; left time: 8.0393s
	iters: 1900, epoch: 1 | loss: 0.5428450
	speed: 0.0055s/iter; left time: 6.8709s
	iters: 2000, epoch: 1 | loss: 0.6564373
	speed: 0.0051s/iter; left time: 5.8907s
	iters: 2100, epoch: 1 | loss: 0.7049844
	speed: 0.0051s/iter; left time: 5.4219s
	iters: 2200, epoch: 1 | loss: 0.8676545
	speed: 0.0049s/iter; left time: 4.6595s
	iters: 2300, epoch: 1 | loss: 0.6093239
	speed: 0.0056s/iter; left time: 4.7824s
	iters: 2400, epoch: 1 | loss: 0.8120917
	speed: 0.0069s/iter; left time: 5.2392s
	iters: 2500, epoch: 1 | loss: 0.5959947
	speed: 0.0059s/iter; left time: 3.9129s
	iters: 2600, epoch: 1 | loss: 0.6610805
	speed: 0.0051s/iter; left time: 2.8502s
	iters: 2700, epoch: 1 | loss: 0.5509281
	speed: 0.0051s/iter; left time: 2.3540s
	iters: 2800, epoch: 1 | loss: 0.6585106
	speed: 0.0056s/iter; left time: 2.0034s
	iters: 2900, epoch: 1 | loss: 0.6883755
	speed: 0.0056s/iter; left time: 1.4494s
	iters: 3000, epoch: 1 | loss: 0.6444125
	speed: 0.0051s/iter; left time: 0.8132s
	iters: 3100, epoch: 1 | loss: 0.5743778
	speed: 0.0050s/iter; left time: 0.2953s
Epoch: 1 cost time: 18.655028104782104
Epoch: 1, Steps: 3158 | Train Loss: 0.7039107 Vali Loss: 0.0153795 Test Loss: 0.5111126
Validation loss decreased (inf --> 0.015379).  Saving model ...
Updating learning rate to 0.0001
>>>>>>>testing : data_teatro_romano_DLinear_custom_ftMS_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 57716
mse:0.5111125111579895, mae:0.33175796270370483, rse:0.8977492451667786, corr:[0.40579668 0.40716505 0.40880865 0.40957323 0.41410872 0.41278595
 0.41360563 0.41189772 0.4066251  0.40688345 0.40628904 0.4023494
 0.40245378 0.401284   0.40404585 0.40303552 0.40605342 0.40968856
 0.4092736  0.4118549  0.4141275  0.4116777  0.41381854 0.41687357
 0.41804728 0.41532046 0.41734368 0.41428626 0.41480052 0.4173103
 0.41515785 0.4170091  0.41518787 0.41395313 0.41435897 0.41578335
 0.41545227 0.41578814 0.41625392 0.41384587 0.4156252  0.41338745
 0.41530573 0.41571814 0.41428432 0.41664252 0.41523612 0.41942087
 0.41575533 0.41569182 0.41636077 0.41928187 0.4174842  0.4168233
 0.4173779  0.41504186 0.41418245 0.41511944 0.41369194 0.416742
 0.41596815 0.4155231  0.41589838 0.41685244 0.4177512  0.41658005
 0.4183494  0.41747147 0.41765243 0.41823268 0.41586536 0.41701943
 0.41755977 0.41413847 0.41340598 0.41690603 0.4147681  0.41586375
 0.41578186 0.41287127 0.41354564 0.41013956 0.4098271  0.41075394
 0.40960956 0.40866324 0.40947083 0.40856165 0.40771765 0.41241786
 0.41811064 0.41724235 0.42471942 0.4314226  0.43352637 0.44082534]
